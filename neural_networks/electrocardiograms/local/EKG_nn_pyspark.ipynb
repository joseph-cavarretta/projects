{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark\n",
    "# !pip install findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import isnull\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../data/mitbih_train.csv'\n",
    "TEST_PATH = '../data/mitbih_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_set = spark.read.csv(TRAIN_PATH, header=False, inferSchema=True).cache()\n",
    "test_set = spark.read.csv(TEST_PATH, header=False, inferSchema=True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for columns with at least one null value\n",
    "assert len([x for x in train_set.columns if train_set.filter(col(x).isNull()).count() > 0]) == 0\n",
    "assert len([x for x in test_set.columns if test_set.filter(col(x).isNull()).count() > 0]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape:  87554 188\n",
      "Test Data Shape:  21892 188\n"
     ]
    }
   ],
   "source": [
    "print('Training Data Shape: ', train_set.count(), len(train_set.columns))\n",
    "print('Test Data Shape: ', test_set.count(), len(test_set.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|_c187|count|\n",
      "+-----+-----+\n",
      "|  0.0|72471|\n",
      "|  4.0| 6431|\n",
      "|  2.0| 5788|\n",
      "|  1.0| 2223|\n",
      "|  3.0|  641|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# class balance\n",
    "train_set.groupBy(train_set.columns[187]).count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes: ['N': 0, 'S': 1, 'V': 2, 'F': 3, 'Q': 4]\n",
    "# 0=normal (N), 1=SVEB (S), 2=VEB (V), 3=Fusion beat (F), 4=Unknown beat (Q)\n",
    "\n",
    "\"\"\"\n",
    "• N: Normal beat\n",
    "• S: Supraventricular premature beat\n",
    "• V: Premature ventricular contraction\n",
    "• F: Fusion of ventricular and normal beat\n",
    "• Q: Unclassifiable beat\n",
    "\n",
    "N\n",
    "Normal\n",
    "Left/Right bundle branch block\n",
    "Atrial escape\n",
    "Nodal escape\n",
    "\n",
    "S\n",
    "Atrial premature\n",
    "Aberrant atrial premature\n",
    "Nodal premature\n",
    "Supra-ventricular premature\n",
    "\n",
    "V\n",
    "Premature ventricular contraction\n",
    "Ventricular escape\n",
    "\n",
    "F\n",
    "Fusion of ventricular and normal\n",
    "\n",
    "Q\n",
    "Paced\n",
    "Fusion of paced and normal\n",
    "Unclassifiable\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = train_set.select(train_set.columns[:-1])\n",
    "label_col = train_set.select(train_set.columns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(\n",
    "    inputCol='_c187', \n",
    "    outputCol=\"label\"\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols.columns,\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "layers = [187, 75, 75, 75, 5]\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    maxIter=300, \n",
    "    layers=layers, \n",
    "    blockSize=128, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, assembler, mlp])\n",
    "mlpModel = pipeline.fit(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_cols = test_set.select(test_set.columns[:-1])\n",
    "test_label_col = test_set.select(test_set.columns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlpModel.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'features', 'rawPrediction', 'probability', 'prediction']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.columns[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionAndLabels = predictions.select(\"prediction\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_f1 = MulticlassClassificationEvaluator(metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set f1 = 0.9713789511997215\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set f1 = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c57e946eca0501ec085bb6c62d03ab28c9db645d5ac24433c1c7b3f8db8f226"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
